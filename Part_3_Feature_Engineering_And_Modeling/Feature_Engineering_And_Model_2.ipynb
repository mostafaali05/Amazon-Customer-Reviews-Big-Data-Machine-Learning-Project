{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44d09b2-a787-477a-a2e7-9cfdafd7c7b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, OneHotEncoder, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f367d193-dad7-47a0-afb6-b08b0f93c9a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"FeatureEngineering\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8c6b61-db9d-4d4d-bdb5-820d30d7c030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading back the cleaned data from the 'raw' folder\n",
    "parquet_file_path = \"s3://amazon-reviews-ma/raw/cleaned_amazon_reviews_us_Apparel_v1_00.parquet\"\n",
    "sdf = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2890e1c-99b7-4adc-9ada-337011fa393d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "# Initializing Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"SimplifiedClassificationModeling\") \\\n",
    ".getOrCreate()\n",
    "# List of file paths\n",
    "file_paths = [\n",
    "# \"s3://amazon-reviews-ma/raw/cleaned_amazon_reviews_us_Apparel_v1_00.parquet\",\n",
    "\"s3://amazon-reviews-ma/raw/cleaned_amazon_reviews_us_Digital_Music_Purchase_v1_00.parquet\",\n",
    "# \"s3://amazon-reviews-ma/raw/cleaned_amazon_reviews_us_Baby_v1_00.parquet\",\n",
    "\"s3://amazon-reviews-ma/raw/cleaned_amazon_reviews_us_Digital_Software_v1_00.parquet\"\n",
    "# more files if I want to add\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5a68a9-148d-4a1b-ae86-2dd4b0ffaff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to read and sample a parquet file\n",
    "def read_and_sample(file_path, fraction=0.1, seed=42):\n",
    "    df = spark.read.parquet(file_path)\n",
    "    return df.sample(withReplacement=False, fraction=fraction, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ec5052-3983-4b16-b19f-31d12ae9e6ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and sample each DataFrame\n",
    "sampled_dfs = [read_and_sample(file_path) for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b31a23-e949-401f-a688-9e5c7156702d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to union two DataFrames\n",
    "def union_dfs(df1, df2):\n",
    "    return df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8def5082-d2f4-4071-800a-17277290f055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine all DataFrames\n",
    "sdf = reduce(union_dfs, sampled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf685eb-3a8d-4b12-8ead-fee5dfc0f19f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'star_rating' into a binary label: 1 for ratings > 3, 0 for ratings <= 3\n",
    "sdf = sdf.withColumn(\"label\", when(col(\"star_rating\") > 3, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509fef77-b466-4297-a749-1b3039b77a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process text data for 'cleaned_review_body'\n",
    "bodyTokenizer = Tokenizer(inputCol=\"clean_review_body\", outputCol=\"tokenized_body\")\n",
    "bodyRemover = StopWordsRemover(inputCol=\"tokenized_body\", outputCol=\"filtered_body\")\n",
    "bodyHashingTF = HashingTF(inputCol=\"filtered_body\", outputCol=\"body_features\")\n",
    "bodyIdf = IDF(inputCol=\"body_features\", outputCol=\"final_body_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fc9c9e-394b-4b52-a263-ac718aa909de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process text data for 'cleaned_review_headline'\n",
    "headlineTokenizer = Tokenizer(inputCol=\"clean_review_headline\", \n",
    "outputCol=\"tokenized_headline\")\n",
    "headlineRemover = StopWordsRemover(inputCol=\"tokenized_headline\", \n",
    "outputCol=\"filtered_headline\")\n",
    "headlineHashingTF = HashingTF(inputCol=\"filtered_headline\", \n",
    "outputCol=\"headline_features\")\n",
    "headlineIdf = IDF(inputCol=\"headline_features\", outputCol=\"final_headline_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f113e792-7843-4229-8308-92de87ce9afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# StringIndexer for the 'product_category' column\n",
    "categoryIndexer = StringIndexer(inputCol=\"product_category\", \n",
    "outputCol=\"categoryIndex\")\n",
    "categoryEncoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271900dd-dca1-4153-9f12-f2ee3b7aeb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble all features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"final_body_features\", \"final_headline_features\", \"categoryVec\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c443dd-716c-4adc-a76e-4100732af3a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the classification model\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87effbd2-9082-44ee-b80a-972942d9f07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Pipeline that combines all the stages\n",
    "pipeline = Pipeline(stages=[\n",
    "bodyTokenizer, bodyRemover, bodyHashingTF, bodyIdf,\n",
    "headlineTokenizer, headlineRemover, headlineHashingTF, headlineIdf,\n",
    "categoryIndexer, categoryEncoder,\n",
    "assembler, classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d612e0-6f4e-4871-a414-3f485064d680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spliting data\n",
    "train_data, test_data = sdf.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b3f788-8fdf-4511-844c-88076a41b12d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fiting the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9beb9ccd-9d25-4498-916f-37084c3d68a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3256e78a-93c6-4263-9877-f4ad0527cd31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluating the model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"Test Area Under ROC: \" + str(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4e29af-a0d2-49cf-a895-e8763e740aae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# Accuracy, Precision, Recall, and F1 Score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79cfe043-50c8-46f1-a82b-3963adc64992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b32a520-7b0e-4bbe-b38f-9c15466da49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix.ravel()\n",
    "import pandas as pd\n",
    "conf_matrix_df = pd.DataFrame(confusion_matrix, \n",
    "index=[\"Actual Negative\", \"Actual Positive\"], \n",
    "columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0db28ea-d6a5-461d-8947-15fa1bcced5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving the model to S3\n",
    "model.write().overwrite().save(\"s3://amazon-reviews-ma/models/myAmazonLogisticRegressionModel2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
